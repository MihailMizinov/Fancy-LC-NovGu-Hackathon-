import numpy as np
import faiss
import json
import os
from sentence_transformers import SentenceTransformer
import torch
from sklearn.metrics.pairwise import cosine_similarity
import config
import time
import requests
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter


class RetrievalSystem:
    def __init__(self, model_name=None):
        if model_name is None:
            model_name = config.SYSTEM_CONFIG['retrieval']['model']
        
        print("üîß –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        self.model = self._load_model_with_retry(model_name)
        
        self.index = None
        self.chunks = []
        self.metadata = []

    def _load_model_with_retry(self, model_name, max_retries=3, retry_delay=10):
        for attempt in range(max_retries):
            try:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏...")
                session = requests.Session()
                retry_strategy = Retry(
                    total=3,
                    backoff_factor=1,
                    status_forcelist=[429, 500, 502, 503, 504],
                )
                adapter = HTTPAdapter(max_retries=retry_strategy)
                session.mount("http://", adapter)
                session.mount("https://", adapter)
                
                model = SentenceTransformer(model_name)
                print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return model
                
            except requests.exceptions.ChunkedEncodingError as e:
                print(f"‚ùå –û–±—Ä—ã–≤ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    print(f"üïí –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {retry_delay} —Å–µ–∫...")
                    time.sleep(retry_delay)
                    retry_delay *= 2
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    raise e
        raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")

    def build_index(self, chunks, index_path):
        print("üî® –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        texts = [chunk['text'] for chunk in chunks]
        embeddings = self.model.encode(texts, show_progress_bar=True, batch_size=32)

        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype(np.float32))
        
        self.chunks = chunks
        self.metadata = chunks
        
        os.makedirs(index_path, exist_ok=True)
        faiss.write_index(self.index, f"{index_path}/faiss.index")
        with open(f"{index_path}/metadata.json", 'w', encoding='utf-8') as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω. –ß–∞–Ω–∫–æ–≤: {len(chunks)}")

    def load_index(self, index_path):
        self.index = faiss.read_index(f"{index_path}/faiss.index")
        with open(f"{index_path}/metadata.json", 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω. –ß–∞–Ω–∫–æ–≤: {len(self.metadata)}")

    def search(self, query, top_k=None, similarity_threshold=None):
        if top_k is None:
            top_k = config.SYSTEM_CONFIG['retrieval']['top_k']          # 8
        if similarity_threshold is None:
            similarity_threshold = config.SYSTEM_CONFIG['retrieval']['similarity_threshold']  # 0.5

        print(f"üîç –ü–æ–∏—Å–∫: top_k={top_k}, threshold={similarity_threshold}")

        if self.index is None:
            raise ValueError("–ò–Ω–¥–µ–∫—Å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        query_embedding = self.model.encode([query])
        faiss.normalize_L2(query_embedding)

        # –ò—â–µ–º –≤ 3 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, —á—Ç–æ–±—ã –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞ –æ—Å—Ç–∞–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
        search_k = min(top_k * 3, len(self.metadata))
        scores, indices = self.index.search(query_embedding.astype(np.float32), search_k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx >= len(self.metadata):
                continue
            if score >= similarity_threshold:
                results.append({
                    'text': self.metadata[idx]['text'],
                    'source': self.metadata[idx]['source'],
                    'page': self.metadata[idx].get('page', 1),
                    'similarity': float(score)
                })
                if len(results) >= top_k:
                    break

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {len(results)}")
        return results[:top_k]

    def calculate_confidence(self, query, context_chunks):
        if not context_chunks:
            return 0.0
        
        avg_similarity = sum(chunk['similarity'] for chunk in context_chunks) / len(context_chunks)
        
        context_text = " ".join([chunk['text'] for chunk in context_chunks])
        query_emb = self.model.encode([query])
        context_emb = self.model.encode([context_text])
        cross_sim = cosine_similarity(query_emb, context_emb)[0][0]
        
        confidence = (avg_similarity + cross_sim) / 2
        return round(confidence, 4)